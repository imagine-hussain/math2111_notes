
\section{Differentiation}

\subsection{Differentiability, Derivatives and Affine Approximations}

\paragraph{Differentiability in \(\mathbb{R}\)}
A function \(f: \mathbb{R}\to \mathbb{R}\) being differentiable at some \(a\in \mathbb{R}\)
implies that there exists a \textit{good} straight-line approximation to \(f\) at \(a\) called a \textit{tangent line}.
This function may be found as
\[T(x) = f(a) + f'(a)(x-a) = f(a) -f'(a)a + f'(a)x = y_0 + L(x)\]
where for all \(a\), \(y= f(a) - f'(a)a\) and \(L: \mathbb{R}\to \mathbb{R} = f'(a)x\).

Recall that \[f'(a) = \lim_{x\to a} \frac{f(x) - f(a)}{x-a}\].

\paragraph{Affine Maps}
A function \(T: \mathbb{R}^n \to \mathbb{R}^m\) being affine means that there exists
a \(y_0\) such that for all \(x\in \mathbb{R}^n\)
\[T(x) = y_0 + L(x)\].

In \(T: \mathbb{R}\to \mathbb{R}\) this sis of the form \(y = mx+b\).

A function \(f: \mathbb{R}\to \mathbb{R}\) is differentiable if there is 
a good affine approximation to \(f\) of the form
\[T(x) = f(a) - f'(a)a + f'(a)x.\]
In this context good implies that \(f'(x)\) is defined in the usual manner
and exists.

\paragraph{Differentiability in \(\mathbb{R}^n\to \mathbb{R}^n\)}

A function \(f: \Omega\subset \mathbb{R}^n \to \mathbb{R}^m\) is differentiable for some
\(a\in\Omega\) if there exists a linear map \(L: \mathbb{R}n\to \mathbb{R}^m\)
such that
\[
\lim_{x\to a} \frac{
    \left|\left|f(x) - f(a) -L(x-a)\right|\right|
} {
    \left|\left|L(x-a)\right|\right|
} = 0.
\]

Notation: the matrix of the linear map \(L\), the derivative of \(f\) at
\(a\) is denoted by \(D_af\).

\paragraph{Delta Epsilon Definition of Differentiability}
A function \(f: \Omega\subset \mathbb{R}\to \mathbb{R}^m\) is 
differentiable on \(a\in \Omega\) if there is a linear map \(L: \mathbb{R}^n\to \mathbb{R}^m\)
such that \(\forall \epsilon > 0 \exists \delta > 0 \)
such that for all \(x\in \Omega\)
\[
\left|\left|x - a\right|\right| < \delta
\Rightarrow
\left|\left|f(x) - f(a) - L(x-a)\right|\right|
< \epsilon\left|\left|x - a\right|\right|. 
\]

\paragraph{Clairaut's Theorem / Mixed Derivative Theorem}
Suppose \(
f, \frac{\partial f}{\partial x_i}, \frac{\partial f}{\partial x_j},
    \frac{\partial^2 f}{\partial x_i \partial x_j},
    \frac{\partial^2 f}{\partial x_j \partial x_i}
\)
all exist and are continuous on an open set around \(a\) then
\[
    \frac{\partial^2 f}{\partial x_i \partial x_j}
    =
    \frac{\partial^2 f}{\partial x_j \partial x_i}.
\]
That is, the partial derivatives commute.

\paragraph{Differentiability and Continuity} Differentiability implies continuity.
However, continuity does not imply differentiability.
The proof of this is contingent on the fact that for \(x\in \mathbb{R}^n\)
and a \(m\times n\) matrix \(L\)
\[\lim x\to 0 \left|\left|Lx\right|\right| = 0.\]

\paragraph{Partial Derivatives and Differentiability}
Suppose that \(\Omega\subset \mathbb{R}^n\) is open and \(f: \Omega \to \mathbb{R}^m\).
If all partial derivatives \(\dfrac{\partial f_j}{\partial x_i}\) exist
for integers \(i \in [1, n]\), \(j\in [1, m]\) then
\(f\) is differentiable on \(\Omega\).

\subsection{Gradients, Affine Approximations and Matrices}

\paragraph{Jacobian Matrices}
Suppose that all partial derivatives of \(f:\Omega \subset \mathbb{R}^n \to \mathbb{R}^m\)
exist for some \(a\in \Omega\). Then, the Jacobian matrix of \(f\)
\[
    J_a f = 
    \begin{pmatrix}
    \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_1}{\partial x_2} \cdots & \dfrac{\partial f_1}{\partial x_n} \\
    \dfrac{\partial f_2}{\partial x_1} & \dfrac{\partial f_2}{\partial x_2} \cdots & \dfrac{\partial f_2}{\partial x_n} \\
    \vdots & \vdots & \ddots \vdots \\
    \dfrac{\partial f_n}{\partial x_1} & \dfrac{\partial f_n}{\partial x_2} \cdots & \dfrac{\partial f_m}{\partial x_n} \\
    \end{pmatrix}
\]
may be evaluated at a point \(a\).
Where \(f\) is differentiable, its derivative is given by the Jacobian matrix.

Note however, that the Jacobian Matrix may exist even where \(f\) is not differentiable.

\subsection{Gradients, Tangent Planes and Affine Approximations}

\paragraph{Gradient}
For \(f: \Omega\subset \mathbb{R}^n\to \mathbb{R}\), if the Jacobian exists,
then it is given by the \(1\times n\) matrix
\[
    Jf = \begin{pmatrix}
        \dfrac{\partial f}{\partial x_1} \\
        \dfrac{\partial f}{\partial x_2} \\
        \cdots                           \\
        \dfrac{\partial f}{\partial x_n} \\
    \end{pmatrix}.
\]
This is equivalent to the gradient of \(f\). That is,
\[
    \text{grad}(f) = \grad f = 
    \begin{pmatrix}
        \dfrac{\partial f}{\partial x_1} \\
        \dfrac{\partial f}{\partial x_2} \\
        \cdots                           \\
        \dfrac{\partial f}{\partial x_n} \\
    \end{pmatrix}.
\]

\paragraph{Affine Approximations}
Allow \(f:\Omega\subset\mathbb{R}^n \to \mathbb{R}\) to be a differentiable
function at \(a\in \Omega\).
The best affine approximation to \(f\) at \(a\) may be written in terms of
the gradient vector as
\[
    T(x) = f(a) + \grad f(a) \cdot (x-a).
\]

\paragraph{Tangent Planes}
The tangent plane to a function \(z = f(x, y)\) is given by
\[ z = T(x, y).\]

\subsection{Chain Rule, Directional Derivatives and Tangent Planes}

\paragraph{Chain Rule}
Suppose that \(f: \Omega \subset \mathbb{R}^n \to \mathbb{R}^m\) and
\(g: \Omega' \subset \mathbb{R}^m \to \mathbb{R}^P\) where
\(f(\Omega) = \Omega'\).
If \(f\) and \(g\) are both differentiable then, so is
\(g\circ f: \Omega \to \mathbb{R}^p\) such that
\[
    D_a (g\circ f) = D_(f_(a))g D_a f.
\]
Equivalently,
\[
    D(g\circ f)(a) = Dg(f(a)) Df(a).
\]

\paragraph{Directional Derivative}
The directional derivative of 
\(f: \Omega \subset \mathbb{R}^n \to \mathbb{R}^m\)
in the direction of the unit vector \(u\) at a point \(a\in\Omega\) is
\[
    D_u f(a) = f_{u}'(a)
    =
    \lim_{t\to 0}  \dfrac{f(a + t u) - f(a)}{t}.
\]

Equivalently, if \(f: \Omega \subset \mathbb{R}^n \to \mathbb{R}\)
is differentiable at \(a\) then for a unit vector \(u\)
\[
    D_u f(a) = Df(a) \cdot u = \grad f(a)\cdot u.
\]

Alternatively, allowing \(\theta\) to be the angle between
\(\grad f(a)\) and \(u\),
\[D_u f(a) = |\grad f(a)| \cdot |u| \cdot \cos\theta.\]

\paragraph{Tangent Planes}

Consider the surface in \(\mathbb{R}^3\) defined by \(\phi (x, y, z) = \lambda\).
where \(\lambda\) is constant and \(\phi\) is differentiable.

Let \(c(t) = \left(c_1(t), c_2(t), c_3(t)\right)\) be a differentiable curve
lying on the vector space with a tangent vector given by 
\(c'(t) = \left(c'_1(t), c'_2(t), c'_3(t)\right)\).

Since all points \(c(t)\) lie on the surface, \(\phi(c(t)) = \lambda\).
Thus,
\[
    D(\phi (c(t))) Dc(t) = 0 \Rightarrow \grad \phi c'(t) = 0.
\]
Therefore, all curves passing through a point \(P\) on the surface have
tangent vector normal to \(\grad \phi\). Thus, they all lie in the tangent
plane at \(P\).


\subsection{Taylor Series and Theorem}

\paragraph{Taylor's Theorem}
For all continuous and differentiable functions \(f: \mathbb{R}\to \mathbb{R}\),
\[
    f(x)
    \approx
    P_k(a) = 
    \sum_{n=0}^{k} \frac{1}{n!}f^{(n)}(x) (x-a)^n.
    + R
\]
where the remainder \(R\) is
\[
    R = \frac{1}{(k+1)!} f^{(k+1)}(z) (x-a)^k.
\]
for some \(z\) between \(x\) and \(a\).

\(P_0, P_1,  P_2, P_3\) are the best constant, affine, quadratic, cubic approximations.

\paragraph{Generalizing Taylor's Theorem}
Let \(f: \Omega\subset \mathbb{R}^n \to \mathbb{R}\) be \(C^r\) on the 
open set \(\Omega\).
Let \(a\in \Omega\) be such that the line segment joining \(a\) and \(x\)
lies entirely in \(\Omega\).  Then,
\[f(x) = P_{r, a}(x) + R_{r, a}(a)\]
where for some \(z\) on the line segment between \(a\) and \(x\)
\[ 
    P_{r,a}(x) = f(a) + \sum_{k=1}^{r-1} \frac{1}{k!} D^k f(a)\cdot(x-a)^k, 
    \quad,
    R_{r,a}(a) = \frac{1}{r!}D^r f(z) \cdot (x-a)^r
\]
Note that \(\cdot\) is not a dot product.

\paragraph{Second Degree Taylor Series}
Let \(f: \Omega\subset \mathbb{R}^n \to \mathbb{R}\) be \(C^3\) on \(\Omega\).
Then, ignoring the remainder,
\[
    f(x) = f(a) + \grad f(a)\cdot(x-a) 
    +
    \frac{1}{2} \left((x-a)\cdot \left(
        Hf(a)\cdot(x-a).
    \right) \right)
\]
where \(H\) is the Hessian Matrix.

\subsection{Hessian Matrix and Stationary Points}

\paragraph{Hessian Matrix}
For \(f: \Omega \subset \mathbb{R}^n \to \mathbb{R}\),
the Hessian matrix of \(f\) at a point \(a\) is the \(n\times n\)
matrix
\[
    H(f, a) =
    \begin{pmatrix}
        \dfrac{\partial^2 f}{\partial x_1 \partial x_1} & \dfrac{\partial^2 f}{\partial x_2 \partial x_1} & \cdots &\dfrac{\partial^2 f}{\partial x_n \partial x_1} \\
        \dfrac{\partial^2 f}{\partial x_1 \partial x_2} & \dfrac{\partial^2 f}{\partial x_2 \partial x_2} & \cdots &\dfrac{\partial^2 f}{\partial x_n \partial x_2} \\
        \vdots & \vdots & \ddots & \vdots \\
        \dfrac{\partial^2 f}{\partial x_1 \partial x_3} & \dfrac{\partial^2 f}{\partial x_2 \partial x_3} & \cdots &\dfrac{\partial^2 f}{\partial x_n \partial x_1} \\
    \end{pmatrix}.
\]

\paragraph{Revision: Trace and Determinant}
Recall that the trace is the sum of its diagonal values. The trace of a hessian
matrix is also the sum of its eigenvalues.
Also, the determinant is the product of the eigenvalues.
The eigenvalues of a matrix \(A\) can be found by calculating solutions to
\[\left| A - \lambda I\right| = 0.\]

\paragraph{Definite and Semi Definite Matrices}
For a \(n\times n\) symmetric matrix \(H\),
\begin{itemize}
    \item All eigenvalues are \(> 0\)    \(\Leftrightarrow\) positive definite
    \item All eigenvalues are \(< 0\)    \(\Leftrightarrow\) negative definite
    \item All eigenvalues are \(\geq 0\) \(\Leftrightarrow\) positive semidefinite
    \item All eigenvalues are \(\leq 0\) \(\Leftrightarrow\) negative semidefinite
\end{itemize}

\paragraph{Sylvester's Criterion (for the Definite Property)}
Allow \(H_k\) to be the upper left \(k\times k\) sub-matrix of \(h\)
and let \(\Delta_k = \det H_k\).
Then,
\begin{itemize}
    \item positive definite        \quad \(\Leftrightarrow\) \(\Delta_k > 0 \forall k\)
    \item positive semidefinite    \quad \(\Leftrightarrow\) \(\Delta_k \leq 0 \forall k\)
    \item negative definite        \quad \(\Leftrightarrow\) \(\Delta_k < 0\)  for all odd \(k\) and \(\Delta > 0\) for all even \(k\)
    \item negative semidefinite        \quad \(\Leftrightarrow\) \(\Delta_k \leq 0\)  for all odd \(k\) and \(\Delta \geq 0\) for all even \(k\)
\end{itemize}

\paragraph{The Definite Property and Classification of Stationary Points}
Suppose that \(f: \Omega\subset \mathbb{R}^n\to \mathbb{R}\) is \(C^2\)
and \(\grad f (a) = 0\) at an interior point \(a\) of \(\Omega\).
Then
\begin{itemize}
    \item \(H(f, a)\) is a positive definite \(\Rightarrow\) \(f\) has a local maximum at \(a\)
    \item \(H(f, a)\) is a negative definite \(\Rightarrow\) \(f\) has a local minimum at \(a\)
    \item \(f\) has a local minimum at \(a\) \(\Rightarrow\) \(H(f,a)\) is a positive semidefinite
    \item \(f\) has a local maximum at \(a\) \(\Rightarrow\) \(H(f,a)\) is a negative semidefinite
\end{itemize}
Observe carefully that the semidefinite cases can also be saddle points.

\subsection{Lagrange Multipliers, Implicit and Inverse Function Theorems}

\paragraph{Lagrange Multipliers}
Consider two differentiable functions
\(f: \mathbb{R}^n \to \mathbb{R} \) and \(f: \mathbb{R}^n \to \mathbb{R}\).
Lagrange multipliers are useful for finding local extrema of \(f\)
under the constraint \(S = \{x \in\mathbb{R}^n : g(x) = c, c\in \mathbb{R}\}\).

Then, if a local minimum or maximum of \(f\) occurs on \(a\in S\) then,
\(\grad f()\) and \(\grad g(a)\) are parallel.
That is, when \(\grad g(a) \neq 0\), there exists \(\lambda \in \mathbb{R}\)
such that
\[
    \grad f (a) = \lambda \grad g(a).
\]

Note that this theorem will only provide possible candidates for minimum or maximum
points. There is no guarantee that there exists minimum or maximums of \(f\) on \(S\).

\paragraph{Inverse Function Theorem in \(\mathbb{R}\to \mathbb{R}\)}
Suppose that \(f: \mathbb{R}\to \mathbb{R}\) is differentiable on an
interval \(I\subset \mathbb{R}\) and \(f'(x)\neq 0\) for all \(x\in I.\)
Then, \(f\) is invertible on \(I\) and the inverse \( (f^{-1})'(x) \)

is differentiable such that
\[
    (f^{-1})'(x) = \dfrac{1}{f'(f^{-1}(x))}.
\]

That is, if \(y = f(x)\) and \(f^{-1}\) exist and is differentiable with \(x = f^{-1}(y)\)
then
\[ \frac{dx}{dy} = \frac{1}{\frac{dy}{dx}}. \]

\paragraph{Generalising the Inverse Function Theorem}
Let \(\Omega\subset \mathbb{R}^n\) be open and \(f: \Omega\to \mathbb{R}^n\)
be \(C^1\). Suppose that \(a\in \Omega\).

If the matrix \(Df(a)\) is invertible, then \(f\) is invertible on an open
set \(U\) containing \(a\).
That is, the inverse exists as
\[f^{-1}: f(U)\to U.\]

Further, \(f^{-1}\) is \(C^1\) and for \(x\in U\),
\[
    D_{f(x)}f^{-1} = (D_xf)^{-1}.
\]

Consequently, \(f^{-1}\) has its best affine approximation at \(f(a)\)
as
\[
    f^{-1}(x) \approx a + (D_f)^{-1}(x - f(a)).
\]

\paragraph{Implicit Function Theorem}
% TODO: #11 Implicit Function Theorem



